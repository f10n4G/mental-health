# -*- coding: utf-8 -*-
"""Dataset2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NYCI-TbpMI-UsI_sBkAoDgshk3s3oVPc

## DATA UNDERTANDING

### IMPORT LIBRARIES
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
sns.set(style="whitegrid")

"""### LOAD DATA"""

df = pd.read_csv("student_depression_dataset.csv")
df.head()

"""### DATA STRUCTURE OVERVIEW"""

print("TAIL = ")
display(df.tail())

print("\n DATA INFO = ")
print(df.info())

print("\n DATA SHAPE = ")
print(df.shape)

print("\n DATA COLUMNS = ")
print(df.columns)

"""### DATA TYPES (NUMERIC-CATEGORICAL)"""

print("\n DATA TYPES = ")
print(df.dtypes)

num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.tolist()

print("\n NUMERIC COLUMNS = ", num_cols)
print("\n CATEGORICAL COLUMNS = ", cat_cols)

"""### MISSING VALUE ANALYSIS"""

print("\nTOTAL MISSING VALUES")
print(df.isnull().sum())

print("\nPERCENT MISSING VALUES (%)")
print(df.isnull().mean() * 100)

plt.figure(figsize=(10,4))
sns.heatmap(df.isnull(), cbar=False, yticklabels=False)
plt.title("Missing Value Heatmap")
plt.show()

"""###  DUPLICATE CHECK"""

print("DUPLICATED ROWS = ")
print(df.duplicated().sum())

"""### DESCRIPTIVE STATISTICS"""

print("\n NUMERICAL SUMMARY")
display(df.describe())

print("\n CATEGORICAL SUMMARY")
display(df.describe(include='object'))

"""### UNIQUE VALUE"""

print("\nUnique Values per Categorical Column:\n")
for col in cat_cols:
    print(f"{col}: {df[col].nunique()} unique values")

"""### DISTRIBUTION ANALYSIS"""

print("\nUnique Values per Numeric Column:\n")
for col in num_cols:
    print(f"{col}: {df[col].nunique()} unique values")

print("\n DISTRIBUTION PLOTS: NUMERICAL")
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(16,12))
axes = axes.flatten()

for i, col in enumerate(num_cols):
    sns.histplot(df[col], kde=True, ax=axes[i], bins=20)
    axes[i].set_title(f"Distribution of {col}")

plt.tight_layout()
plt.show()

print("\n DISTRIBUTION PLOTS: CATEGORICAL")
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,15))
axes = axes.flatten()

for i, col in enumerate(cat_cols):
    sns.countplot(data=df, y=col, ax=axes[i])
    axes[i].set_title(f"Count of {col}")
    axes[i].set_xlabel("Count")

plt.tight_layout()
plt.show()

"""### SKEWNESS CHECK"""

skew_vals = df[num_cols].skew().sort_values(ascending=False)
print(skew_vals)

plt.figure(figsize=(10,5))
sns.barplot(x=skew_vals.index, y=skew_vals.values, palette="magma")
plt.title("Skewness of Numerical Features")
plt.axhline(0, color="black", linewidth=1)
plt.xticks(rotation=45)
plt.show()

"""### OUTLIER DETECTION"""

zscore_outliers = {}
iqr_outliers = {}

for col in num_cols:
    skew = skew_vals[col]
    print(f"\n--- Column: {col} ---")
    print(f"Skewness: {skew:.3f}")

    # Normal → Z-score
    if abs(skew) < 0.5:
        print("Method: Z-score")
        z = np.abs(stats.zscore(df[col]))
        outlier_idx = df[z > 3].index
        zscore_outliers[col] = list(outlier_idx)
        print(f"Outliers detected: {len(outlier_idx)} rows")

    # Skewed → IQR
    else:
        print("Method: IQR")
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outlier_idx = df[(df[col] < lower) | (df[col] > upper)].index
        iqr_outliers[col] = list(outlier_idx)
        print(f"Outliers detected: {len(outlier_idx)} rows")

"""### BOXPLOT NUMERICAL FEATURES"""

n_cols = len(num_cols)
n_rows = (n_cols + 3) // 4

plt.figure(figsize=(16, 4 * n_rows))
for i, col in enumerate(num_cols):
    plt.subplot(n_rows, 4, i+1)
    sns.boxplot(y=df[col], color='lightblue')
    plt.title(f"Boxplot of {col}")

plt.tight_layout()
plt.show()

"""### CORELATION (NUMERIC)"""

plt.figure(figsize=(10,8))
sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""### TARGET IMBALANCE CHECK"""

target_col = "Depression"

print("\nTARGET CLASS DISTRIBUTION")
print(df[target_col].value_counts())

print("\nTARGET CLASS PERCENTAGE")
print((df[target_col].value_counts(normalize=True) * 100).round(2))

plt.figure(figsize=(6,4))
sns.countplot(x=target_col, data=df, palette="coolwarm")
plt.title("Target Distribution: Depression")
plt.xlabel("Depression (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

"""### PAIRPLOT"""

sns.pairplot(df[num_cols])
plt.show()

"""### NUMERIC vs TARGET RELATION (BOXPLOT)"""

print("\nNUMERICAL FEATURES VS TARGET\n")

for col in num_cols:
    if col != target_col:
        plt.figure(figsize=(7,4))
        sns.boxplot(x=target_col, y=col, data=df, palette="Set2")
        plt.title(f"{col} by Depression")
        plt.xlabel("Depression (0 = No, 1 = Yes)")
        plt.show()

ordinal_cols = ["Job Satisfaction",  "Work Pressure"]

for col in ordinal_cols:
    plt.figure(figsize=(7,4))
    sns.stripplot(x=target_col, y=col, data=df, jitter=0.2, palette="Set2")
    plt.title(f"{col} by Depression")
    plt.xlabel("Depression (0 = No, 1 = Yes)")
    plt.show()

"""### CATEGORICAL vs TARGET RELATION"""

for col in cat_cols:
     if col != target_col:
        print(f"\n===== {col} vs Depression =====")
        display(pd.crosstab(df[col], df[target_col], normalize="index").round(2))

        plt.figure(figsize=(8,4))
        sns.countplot(x=col, hue=target_col, data=df, palette="coolwarm")
        plt.title(f"{col} vs Depression")
        plt.xticks(rotation=45)
        plt.legend(title="Depression", labels=["No (0)", "Yes (1)"])
        plt.show()

"""## DATA PREPARATION"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

"""### COLUMN REMOVAL"""

df_prep = df.copy()

if 'id' in df_prep.columns:
    df_prep = df_prep.drop(columns=['id'])


drop_cols = ['Work Pressure', 'Job Satisfaction']
df_prep = df_prep.drop(columns=drop_cols)

"""### CATEGORIZING FEATURES"""

continuous_cols = ['Age', 'CGPA', 'Work/Study Hours']

ordinal_cols = ['Academic Pressure',
                'Study Satisfaction']

categorical_cols = ['Gender',
                    'City',
                    'Profession',
                    'Sleep Duration',
                    'Dietary Habits',
                    'Degree',
                    'Social Weakness',
                    'Have you ever had suicidal thoughts ?',
                    'Financial Stress',
                    'Family History of Mental Illness']

print("Continuous cols :", continuous_cols)
print("Ordinal cols    :", ordinal_cols)
print("Categorical cols:", categorical_cols)

"""### HANDLING MISSING & DUPLICATE VALUES"""

print(df_prep.isnull().sum())
df_prep = df_prep.dropna()

df_prep = df_prep.drop_duplicates()
print("Duplicate rows after removal:", df_prep.duplicated().sum())

"""### ENCODING"""

from sklearn.preprocessing import LabelEncoder
from category_encoders.target_encoder import TargetEncoder

df_encoded = df_prep.copy()

###CLEANING
df_encoded['Sleep Duration'] = (
    df_encoded['Sleep Duration']
    .astype(str)
    .str.replace("'", "")
    .str.strip()
)

#BINARY ENCODING
ordinal_mapping = {
    "Sleep Duration": {
        "Less than 5 hours": 1,
        "5-6 hours": 2,
        "7-8 hours": 3,
        "More than 8 hours": 4,
        "Others": 0
    },
    "Financial Stress": {
        "1.0": 1, "2.0": 2, "3.0": 3, "4.0": 4, "5.0": 5, "?": 0
    },
    "Have you ever had suicidal thoughts ?": {"No": 0, "Yes": 1},
    "Family History of Mental Illness": {"No": 0, "Yes": 1}
}

for col, mapping in ordinal_mapping.items():
    df_encoded[col] = df_encoded[col].map(mapping).fillna(0).astype(float)


#LABEL ENCODING
label_cols = ['Gender', 'Dietary Habits', 'Degree']

print("\nApplying Label Encoding to:", label_cols)

le = LabelEncoder()

for col in label_cols:
    df_encoded[col] = df_encoded[col].astype(str)
    df_encoded[col] = le.fit_transform(df_encoded[col])


#TARGET ENCODING
target_enc_cols = ['City', 'Profession']
target_col = "Depression"

print("\nApplying Target Encoding to:", target_enc_cols)

te = TargetEncoder()
df_encoded[target_enc_cols] = te.fit_transform(df_encoded[target_enc_cols], df_encoded[target_col])


print("\nEncoding Complete")
print("\nData Types After Encoding:\n")
print(df_encoded.dtypes)

print("\nSample Preview:\n")
df_encoded.head()

"""### HANDLING OUTLIERS"""

outlier_cols = ['Age', 'CGPA']

def cap_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    return df

for col in outlier_cols:
    print(f"Applying : {col}")
    df_encoded = cap_outliers(df_encoded, col)

print("\nRechecking outliers after capping:")

for col in outlier_cols:
    Q1 = df_encoded[col].quantile(0.25)
    Q3 = df_encoded[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    outliers = df_encoded[(df_encoded[col] < lower) | (df_encoded[col] > upper)]
    print(f"{col} -> Remaining Outliers: {len(outliers)}")

import matplotlib.pyplot as plt
import seaborn as sns

num_cols = df_encoded.select_dtypes(include=['int64', 'float64']).columns

print(f"Total Numeric Columns: {len(num_cols)}")

plt.figure(figsize=(25, 12))
df_encoded[num_cols].hist(bins=20, figsize=(25, 12), color='steelblue')
plt.suptitle("Histogram After Encoding", fontsize=18)
plt.show()

corr = df_encoded.corr()

plt.figure(figsize=(15, 10))
sns.heatmap(corr, cmap="coolwarm", annot=False, linewidths=0.3)
plt.title("Correlation Heatmap After Encoding", fontsize=16)
plt.show()

target = "Depression"

corr_target = corr[target].sort_values(ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=corr_target.values, y=corr_target.index, palette="coolwarm")
plt.title(f"Correlation of Features to Target ({target})", fontsize=16)
plt.show()

from sklearn.preprocessing import StandardScaler

# Copy dataset for scaling
df_scaled = df_encoded.copy()

# Identify features except target
feature_cols = df_scaled.columns[df_scaled.columns != 'Depression']

# Apply StandardScaler
scaler = StandardScaler()
df_scaled[feature_cols] = scaler.fit_transform(df_scaled[feature_cols])

df_scaled.head()

from sklearn.model_selection import train_test_split

# Pisahkan X dan y
X = df_scaled.drop(columns=['Depression'])
y = df_scaled['Depression']

# Train-test split dengan stratifikasi
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

print("\nDistribusi Target di Training Set:")
print(y_train.value_counts(normalize=True) * 100)

print("\nDistribusi Target di Test Set:")
print(y_test.value_counts(normalize=True) * 100)

#KNN

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Baseline KNN Model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Prediction
y_pred_knn = knn.predict(X_test)

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred_knn)
precision = precision_score(y_test, y_pred_knn)
recall = recall_score(y_test, y_pred_knn)
f1 = f1_score(y_test, y_pred_knn)

print("KNN Baseline Model Performance:")
print(f"Accuracy  : {accuracy:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1-Score  : {f1:.4f}")

# Classification Report
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred_knn))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_knn)
cm

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.title("KNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

#Desicion Tree

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Baseline Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predictions
y_pred_dt = dt.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_dt)
precision = precision_score(y_test, y_pred_dt)
recall = recall_score(y_test, y_pred_dt)
f1 = f1_score(y_test, y_pred_dt)

print("Decision Tree Baseline Performance:")
print(f"Accuracy  : {accuracy:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1-Score  : {f1:.4f}")

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred_dt))

cm = confusion_matrix(y_test, y_pred_dt)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens")
plt.title("Decision Tree Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

svm_model = SVC(kernel='rbf', random_state=42)
svm_model.fit(X_train, y_train)

y_pred_svm = svm_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_svm)
precision = precision_score(y_test, y_pred_svm)
recall = recall_score(y_test, y_pred_svm)
f1 = f1_score(y_test, y_pred_svm)

print("SVM Baseline Model Performance:")
print(f"Accuracy  : {accuracy:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1-score  : {f1:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm))

cm_svm = confusion_matrix(y_test, y_pred_svm)

plt.figure(figsize=(6,4))
sns.heatmap(cm_svm, annot=True, cmap="Blues", fmt="d")
plt.title("SVM Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42
)

rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_rf)
precision = precision_score(y_test, y_pred_rf)
recall = recall_score(y_test, y_pred_rf)
f1 = f1_score(y_test, y_pred_rf)

print("Random Forest Model Performance:")
print(f"Accuracy  : {accuracy:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1-score  : {f1:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

cm_rf = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(6,4))
sns.heatmap(cm_rf, annot=True, cmap="Greens", fmt="d")
plt.title("Random Forest Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

feature_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

feature_importances.head(20)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

# ========== Train Gradient Boosting ==========
gb_model = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=3,
    random_state=42
)

gb_model.fit(X_train_fs, y_train_fs)
y_pred_gb = gb_model.predict(X_test_fs)

accuracy_gb = accuracy_score(y_test_fs, y_pred_gb)
report_gb = classification_report(y_test_fs, y_pred_gb)
conf_matrix_gb = confusion_matrix(y_test_fs, y_pred_gb)
roc_gb = roc_auc_score(y_test_fs, gb_model.predict_proba(X_test_fs)[:,1])

print("\nGradient Boosting Model Results:")
print(f"Accuracy: {accuracy_gb:.4f}")
print(f"ROC-AUC Score: {roc_gb:.4f}")
print("\nClassification Report:")
print(report_gb)

plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix_gb, annot=True, fmt="d", cmap="Greens")
plt.title("Gradient Boosting - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()